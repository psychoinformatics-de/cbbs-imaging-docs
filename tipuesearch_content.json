{"pages":[{"url":"datamanagement/","text":"The fundamental concept of this platform is that data are first transformed into a standard layout using common file formats, so that all processing pipelines can expect a deterministic, well described data structure. This setup enables the development of data processing components that are agnostic of the peculiarities of individual studies, as long as all relevant aspects of a study are properly described. The necessary data conversion and layout are performed largely automatic. The procedures are tuned for data acquired in Magdeburg, but with minor extra effort data form other sources can be processed too. For each study the following steps will be performed: Initial study setup: create a study raw dataset Import a new acquisition (repeated as necessary): Import a DICOM dataset Import other data modalities Complete study description (if necessary): Identify data that were acquired simultaneously during an acquisition Associate stimulation and behavior logs with (MR) data acquisitions Identify data formats/converters for non-standard acquisitions (e.g. custom hardware) Convert raw data into the common data structure Verify result of automatic conversion (automatic validation tools are available) Apply analysis procedure","tags":"pages","title":"Data Management"},{"url":"datamanagement/study_setup/","text":"A study raw dataset contains all raw data and metadata on a study, across all acquisitions and participants. It is typically created just prior the first (pilot) acquisition. Its purpose is to record all data in their raw, unaltered form, as provided by the respective acquisition machinery. Moreover, it is used to bind data from multiple acquisition sources (e.g. MRI, eye tracker, ECG) together, to create a reliable record of what data were recorded at which point on what hardware. Importantly, this also includes the logs of any stimulation setup, or behavior responses that are typically generated by paradigm implementations provided by individual researchers. The study raw dataset should capture all information required to reproduce any subsequently performed data analyses. Preparation Start off by installing https://github.com/psychoinformatics-de/cbbs-imaging-container-import , which also provides additional tools apart from sheer creation: datalad install -s https://github.com/psychoinformatics-de/cbbs-imaging-container-import [INSTALL-DIR] After successful installation run the following command to retrieve the actual container image: datalad get INSTALL-DIR/cbbs-imaging.simg Dataset creation To create a study dataset you just need to run: singularity run INSTALL-DIR/cbbs-imaging.simg create [TARGET-DIR] . If you don't provide a target dir, the dataset will be created in the current working directory.","tags":"datamanagement","title":"Initial study setup"},{"url":"datamanagement/import_dicoms/","text":"The raw data import container also provides a command to import a DICOM tarball for a session into a study dataset . To import such a tarball just run: singularity run [--bind=/path/to/tarball] INSTALL-DIR/cbbs-imaging.simg import [TARBALL] from within your study dataset. This will result in a subdataset with the DICOMs in it, that provides the DICOM metadata for easy access via datalad . Note, that TARBALL needs to be an absolute path at the moment. Particularly if TARBALL is located outside your HOME directory, you may need to provide its location to the container via the --bind option, in case the tarball is on a different (network) drive. A session name like xx99_0123 will be determined from the DICOM metadata ( PatientID ) and a subdirectory with this name is created. Alternatively, a session name can be provided as a second argument to the import command. The subdirectory contains a studyspec.json file with session-related metadata (info on DICOM series to convert to NIfTI, other simultaneously acquired data modalities), and a subdirectory dicoms/ , with all DICOM data and metadata. DICOM subdatasets can be archived separately to, for example, build raw data archives (for a lab, a scanner, an institution) that can be easily queried for scan with particular properties (see this demo for an example).","tags":"datamanagement","title":"Import DICOMs"},{"url":"datamanagement/conversion/","text":"","tags":"datamanagement","title":"Raw Data Conversion"},{"url":"datamanagement/tools/","text":"DataLad Git","tags":"datamanagement","title":"Tools"},{"url":"datamanagement/demo_scandb/","text":"DICOM datasets that have been imported into a study raw dataset can (additionally) be collected in scanner (or institution or lab) specific superdatasets. This allows for convenient record keeping of all relevant MR data acquisitions ever made in a given context. The example script at the bottom of this page shows how to bootstrap such a database. Such superdatasets are lightweight, as they do not contain actual imaging data, and can be queried using a flexible language. In the DICOM context it is often desired to limit the amount of metadata to whole datasets and their image series. This can be achieved using the following configuration, which only needs to be put into the top-most dataset, not every DICOM dataset: % cat .datalad/config [datalad \"dataset\"] id = 349bb81a-1afe-11e8-959f-a0369f7c647e [datalad \"search\"] index-autofield-documenttype = datasets default-mode = autofield With this setup the DataLad search command will automatically discover metadata for any contained image series, and build a search index that can be queried for values in one or more individual DICOM fields. This allows for a variety of useful queries. Example queries Report scans made on any male patients in a given time span: % datalad search dicom.Series.AcquisitionDate:'[20130410 TO 20140101]' dicom.Series.PatientSex:'M' search(ok): lin/7t/xx99_2022/dicoms (dataset) Report any scans for a particular subject ID: % datalad search 'xx99*' [INFO ] Query completed in 0.019682836998981657 sec. Reporting up to 20 top matches. search(ok): lin/7t/xx99_2022/dicoms (dataset) search(ok): lin/7t/xx99_2014/dicoms (dataset) search(ok): lin/7t/xx99_2015/dicoms (dataset) search(ok): lin/3t/xx99_0138/dicoms (dataset) search(ok): lin/3t/xx99_0139/dicoms (dataset) search(ok): lin/3t/xx99_0140/dicoms (dataset) action summary: search (ok: 6) For each search hit ALL available metadata is returned. This allows for sophisticated output formating. Here is an example that reports all studies a particular subject has participated in: % datalad -f '{metadata[dicom][Series][0][StudyDescription]}' search 'xx99*' | uniq [INFO] Query completed in 0.02244874399912078 sec. Reporting up to 20 top matches. Studies&#94;Michael_Hanke transrep2 Transrep2 Demo script to bootstrap a DICOM database from scan tarballs The following script shows how a bunch of DICOM tarballs from two different scanners can be imported into a DataLad superdataset for each scanner. Those two scanner datasets are than assembled into a joint superdataset for acquisition hardware of the institution. Metadata from any acquisition session can then be aggregated into this dataset, to track all acquisitions made on those devices, as well as to be able to query for individual scan sessions, DICOM series, or individual DICOM images (see above for query examples). # get the tools datalad install -s https://github.com/psychoinformatics-de/cbbs-imaging-container-import import datalad get import/cbbs-imaging.simg # create a super dataset that will have all acquisitions the 7T ever made singularity run import/cbbs-imaging.simg create 7t cd 7t # import a bunch of DICOM tarballs (simulates daily routine) singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/pandora/data/xx99/raw/dicom/xx99_2022.20130410.103515.930000.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2014.20130408.123933.087500.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2015.20130408.140515.147500.tar.gz # done for now cd .. # now the same for 3t singularity run import/cbbs-imaging.simg create 3t cd 3t # import a bunch of DICOM tarballs singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0138.20140425.121603.06.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0139.20140425.142752.07.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_visloc/mri/xx99_0140.20140425.155736.23.tar.gz # done cd .. # one dataset for the entire institute's scan (could in turn be part of one that also # includes other modalities/machines) # this first part only needs to be done once datalad create lin cd lin datalad install -d . -s ../7t datalad install -d . -s ../3t # this second part needs to be done everytime the metadata DB shall be updated # get the latest state of the scanner datasets (no heavy stuff is moved around) datalad update --merge -r # aggregate from the aggregated metadata datalad aggregate-metadata -r # ready to search","tags":"datamanagement","title":"Demo: DICOM DB"},{"url":"datamanagement/tools/datalad/","text":"","tags":"tools","title":"Datalad"},{"url":"datamanagement/tools/git/","text":"Git enables you to track the changes made to files over time â€” specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt-get install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Setup Once Git is installed, configure it with your name and email address. This lets Git know who you are so that it can associate you with the commits you make. git config --global user.name \"John Doe\" git config --global user.email johndoe@example.com Basic Commands git init Tells git to enable tracking of changes that happen in this folder. git clone <url> | <user@server:path/to/repo.git> Makes a full copy of an existing git repository â€” all files, folders, changes, history, etc. git status Lists which files are in which state â€” if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made and select which will be staged . git commit Commits all the staged changes (done with git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Stages the file to be removed. After you commit, the file will be removed and no longer tracked. But the file does remain in the project history. git mv <file-from> <file-to> Moves/renames a file. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and will need to be installed ( apt-get install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching if you're still unsure about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","title":"Git"},{"url":"analysis/","text":"","tags":"pages","title":"Data Analysis"},{"url":"devices/","text":"The pages in this section contain information on data acquisition devices (to be) supported in the CBBS imaging platform.","tags":"pages","title":"Acquisition Hardware"},{"url":"devices/magnetom/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"7T Siemens Magnetom"},{"url":"devices/achieva/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Philips Achieva"},{"url":"devices/prisma/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Siemens Prisma"},{"url":"devices/skyra/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Skyra"},{"url":"devices/verio/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Verio"},{"url":"devices/eyelink/","text":"MR-compatible Eyelink 1000 (responsible: Stefan Pollmann). Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Eyelink 1000"},{"url":"devices/stadlerphysio/","text":"This is JÃ¶rg's physio recording box that can be used with the Siemens MR scanners. Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Physio box"},{"url":"containers/","text":"In order to faciliate the reproducibility of research computing, all data processing is performed in containerized computational environments. These environments can be archived alongside the results of the data processing algorithms they have performed, in order to generate a complete record of the processing history. Moreover, such computional environment can be tracked with version control systems to capture which particular results were produced with which software version, in case data processing pipelines have to be adjusted throughout the lifetime of a project. The CBBS imaging platform employs Singularity as the main workhorse for container-based data processing.","tags":"pages","title":"Computational Environments"},{"url":"containers/rawimport/","text":"This Singularity container contains all software necessary to perform raw data import for all supported data types, and for conversion into the common data structure. Development information Report issues","tags":"containers","title":"Raw data import"},{"url":"contributing/","text":"This project welcomes contributions from platform users and developers. The following list provides information on how to contribute to individual aspects of the imaging platform: Contribute to this documentation Contribute an implementation of a new analysis procedure In general, all documentation and source code are available under free and open source licences in various repositories on GitHub . Any contributions in the form of reported issues, or patches are much appreciated.","tags":"pages","title":"Contributing"},{"url":"contributing/docs/","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. If your proposed fix is limited to content, then you can probably skip setting up the build system. You can simply make your changes (content is written in reStructuredText ) and submit a Pull Request . The website is generated by Pelican and the template is written using Jinja2 . So, if your proposed changes are more widespread or change the template, you'll need to setup the build environment. Don't worry; it's easy. install pelican ( apt-get install pelican on Debian; pip install --user pelican on other OSs.) install beautifulsoup4 (again: apt-get install python-beautifulsoup or pip install --user beautifulsoup4 respectively), which is required by some pelican plugins we use run make devserver open a browser to http://127.0.0.1:8000 And that's it. Any changes you make will automatically trigger a rebuild.","tags":"contributing","title":"Documentation"}]}