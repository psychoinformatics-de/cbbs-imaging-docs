{"pages":[{"url":"datamanagement/","text":"The fundamental concept of this platform is that data are first transformed into a standard layout using common file formats, so that all processing pipelines can expect a deterministic, well described data structure. This setup enables the development of data processing components that are agnostic of the peculiarities of individual studies, as long as all relevant aspects of a study are properly described. The necessary data conversion and layout are performed largely automatic. The procedures are tuned for data acquired in Magdeburg, but with minor extra effort data from other sources can be processed too. For each study the following steps will be performed: Initial study setup: create a study raw dataset Import a new acquisition (repeated as necessary): Import a DICOM dataset Import other data modalities Complete study description (if necessary): Identify data that were acquired simultaneously during an acquisition Associate stimulation and behavior logs with (MR) data acquisitions Identify data formats/converters for non-standard acquisitions (e.g. custom hardware) Convert raw data into the common data structure Verify result of automatic conversion (automatic validation tools are available) Apply analysis procedure","tags":"pages","title":"Data Management"},{"url":"datamanagement/study_setup/","text":"A study raw dataset contains all raw data and metadata on a study, across all acquisitions and participants. It is typically created just prior the first (pilot) acquisition. Its purpose is to record all data in their raw, unaltered form, as provided by the respective acquisition machinery. Moreover, it is used to bind data from multiple acquisition sources (e.g. MRI, eye tracker, ECG) together, to create a reliable record of what data were recorded at which point on what hardware. Importantly, this also includes the logs of any stimulation setup, or behavior responses that are typically generated by paradigm implementations provided by individual researchers. The study raw dataset should capture all information required to reproduce any subsequently performed data analyses. A study raw dataset is supposed to have a subdirectory for each acquisition. The name of this directory is the acquisition identifier as far as the tools described here are concerned. Each of those acquisition directories should contain all relevant data of an acquisition. Most notably they contain a subdataset dicoms and a specification file to curate required information for conversion . It is recommended to group other data according to its source and nature similarly (i.e. have a physio subdirectory alongside the dicoms subdirectory). The study raw dataset should also contain custom code needed for conversion in order to provide reproducibility. Preparation Data management relies on datalad and datalad-hirni respectively, which is an extension to datalad to provide additional functionality for this platform. Start off by installing datalad-hirni . Dataset creation To create a study dataset you just need to run: datalad create [TARGET-DIR] If you don't provide a target dir, the dataset will be created in the current working directory. This will create an empty datalad dataset. To preconfigure it to be a study dataset however, you need to run a dedicated setup procedure from within the dataset: cd [TARGET-DIR] datalad run-procedure setup_study_dataset","tags":"datamanagement","title":"Initial study setup"},{"url":"datamanagement/import_dicoms/","text":"datalad-hirni also provides a command to import a DICOM tarball for an acquisition into a study dataset . The command to be used for that is: datalad hirni-import-dcm [TARBALL] [ACQUISITION ID] from within your study dataset. This will result in a subdirectory ACQUISITION ID in your study dataset and a subdataset dicoms beneath with the DICOMs in it, that provides the DICOM metadata for easy access via datalad . In addition a prefilled specification for each series in the tarball is created and stored in the acquisition's specification file . If you don't provide an acquisition identifier, a name like xx99_0123 will be determined from the DICOM metadata. By default this is the value of field PatientID . However, there are special rules in place (and to further be developed) to be applied based on the scanner the data was acquired with. This mechanism allows for different rules per scanner, institution or any other category that can be identified by the DICOM metadata. Use --subject to provide a subject ID. Otherwise the import routine will try to derive the subject ID from the DICOM metadata. A typical case would be the aforementioned acquisition xx99_0123 with a corresponding subject ID xx99 . Optionally, you can use --anon-subject to additionally provide an anonymized subject ID. When converting the dataset to BIDS, the switch --anonymize will then determine which subject ID to use for the converted dataset. Generally, the option --properties allows to add and/or overwrite the specification to be created for this acquisition by passing a JSON string. Thereby you can assign a task label for example: datalad hirni-import-dcm --subject xx99 --anon-subject 007 \\ --properties '{\"bids_task\": \"dofancystuff\", \\ \"comment\": \"something unusual happened during this acquisition\"}' \\ /path/to/tarball DICOM subdatasets can be archived separately to, for example, build raw data archives (for a lab, a scanner, an institution) that can be easily queried for scan with particular properties (see this demo for an example). TODO: Notes on how and what to drop/clean up","tags":"datamanagement","title":"Import DICOMs"},{"url":"datamanagement/import_other/","text":"Apart from DICOM files, you can add any additional data into a study dataset. Information corresponding to any of the MR data acquisitions must be added into the respective directory. This can be stimulation logs, behavioral response logs, or other simultaneously acquired data. To simply add the files to the dataset, copy or move them to the appropriate location and use datalad add to make them part of the dataset. TODO: this prob. needs a link to tools/datalad and an explanation therein However, in order to include the data in the conversion later on, you need to create a specification for it. This is what datalad hirni-spec4anything is for. This will create a snippet for the data component, containing the mandatory pieces and lets you specify all properties of the specification needed for conversion via the same option --properties , that is available when importing DICOM archives . Let's say you added a file physio/df37_200Hz_1.txt to an acquisition. To create a proper specification snippet for that file, from within the acquisition directory you'd call: datalad hirni-spec4anything physio/df37_200Hz_1.txt \\ --properties \"{\\\"converter_path\\\": \\\"../code/convert_physio\", \\\"converter\\\": \\\"{_hs[converter_path]} {_hs[location]} \\ {_hs[bids_subject]} {_hs[bids_task]} \\ {_hs[bids_run]} ${freq}\\\", \\ \\\"bids_run\\\": \\\"01\\\", \\\"type\\\": \\\"physio_file\\\"}\" to create the snippet as shown on the specification page : {\"location\":\"physio/df37_200Hz_1.txt\", \"type\":\"physio_file\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"001\"}, \"converter\":{\"approved\":true,\"value\":\"{_hs[converter_path]} {_hs[location]} {_hs[bids_subject]} {_hs[bids_task]} {_hs[bids_run]}\"}, \"converter_path\":{\"approved\":true,\"value\":\"../code/convert_physio\"}, } Note, that the keys \"subject\", \"anon_subject\", \"bids_task\" were not specified in the call but still created for this snippet. Whenever you use datalad hirni-spec4anything to add a snippet to an acquisition's specification, the command will conclude the values for keys if the value is unambiguous throughout the already existing specification of that acquisition. If there was only one run in the existing data there would also be no need to specify it in the call to spec4anything . There are two fields to specify a custom converter for any data component: converter and converter_path . converter_path contains the path referring to the executable. Note, that this path is relative to the location of the specification file (by default right in the acquisition directory) as is the path to the data itself (key \"location\" in the snippet). The second field converter is a format string specifying how to call that executable to convert the file referred to by this snippet. The curly brackets indicate something to dynamically be replaced. This follows the same concept used by datalad run . As a replacement symbol the dictionary _hs is available. This allows to reference any key in this very snippet. So, {_hs[converter_path]} is replaced by the path to the executable and {_hs[bids_run]} by the value 01 . That way, the string for the converter field can be figured out once per converter and all snippets using it will cause the conversion routine to replace those symbols with the values belonging to the currently converted data component.","tags":"datamanagement","title":"Import additional data"},{"url":"datamanagement/study_specification/","text":"The study specification consists of JSON files in the study raw dataset and provides metadata needed for conversion. The tools provided by datalad-hirni aim to help to (automatically) create and curate those files and convert a study raw dataset (or single acquisition) based on them. Each acquisition in a study raw dataset contains such a file. Its default name is studyspec.json directly underneath the acquisition's directory. It's a JSON-stream consisting of one dictionary per line. We are referring to those dictionaries as snippets . For any data entity in an acquisition (at least any that is to be converted) there should be such a snippet. Those snippets are automatically created when you import a DICOM tarball or import other data into your study raw dataset. However, the automatic creation will likely be incomplete or even incorrect, depending on your needs. Therefore those specifications needs to be edited before converting the dataset. Since those files are simple JSON files, you can change them programmatically, edit the manually with any kind of editor you like or use datalad-hirni's webUI to do so. The specification snippets have keys, that are meant to be edited and some special keys, that are not supposed to be changed, but only to automatically be set on creation. The editable ones have values that are again a dictionary with two keys: value and approved , where approved is a flag signalling whether or not the value is just an automatic guess, that needs to either be confirmed or changed. This is meant to help curation. Generally, a snippet for a dicom series looks like this: {\"location\":\"dicoms\", \"type\":\"dicomseries\", \"uid\":\"1.3.10.2.1102.5.2.34.18716.201347010933155397510580.0.0.0\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_modality\":{\"approved\":false,\"value\":\"bold\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"001\"}, \"converter\":{\"approved\":false,\"value\":\"heudiconv\"}, \"description\":{\"approved\":true,\"value\":\"MoCoSeries_DiCo\"}, \"id\":{\"approved\":false,\"value\":9}, } Note, that the first five entries are aforementioned non-editable fields. They are created by the import routine for DICOMs and supposed to be changed. \"location\", \"type\", \"dataset_id\", \"dataset_refcommit\" are expected in every snippet independent on the type of data this specification is about. \"location\" is a path to the actual data relative to the specification file containing this very snippet. It might refer to a single file or a directory. \"comment\" is field with no implications in how this snippet or the data it is about is dealt with. It is meant to provide a place to put notes regarding the data or TODOs or whatever you might need in order to finish creating the study raw dataset and preparing conversion. All keys starting with bids_ are referring to the terms used in the BIDS standard and are used for conversion. Many of those are optional and therefore can have a value of null or not appear in the specification at all. In this example the values were automatically derived from DICOM metadata (note, that they are not yet \"approved\"), most importantly from the DICOM field ProtocolName . For conversion of DICOMS currently supported BIDS keys are: 'bids_session' 'bids_task' 'bids_run' 'bids_modality' 'bids_acquisition' 'bids_scan' 'bids_contrast_enhancement' 'bids_reconstruction_algorithm' 'bids_echo' 'bids_direction' The value of \"description\" comes from DICOM's SeriesDescription and \"id\" is prefilled with the value of SeriesNumber . It's worth noting, that this \"id\" has no technical meaning for datalad-hirni's commands. It is meant to provide you with a human-readable identification of this DICOM series to ease editing of the snippet. Similarly, a snippet for a physio file may look like this: {\"location\":\"physio/df37_200Hz_1.txt\", \"type\":\"physio_file\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"001\"}, \"converter\":{\"approved\":true,\"value\":\"{_hs[converter_path]} {_hs[location]} {_hs[bids_subject]} {_hs[bids_task]} {_hs[bids_run]}\"}, \"converter_path\":{\"approved\":true,\"value\":\"../code/convert_physio\"}, } Note, that the fields \"converter\" and \"converter_path\" allow to include custom converters for your data by specifying where to find the executable ( code/convert_physio within the study raw dataset in this case) and how to call it in order to convert this particular data. See how to import additional data for further reference on how to determine the values of those two fields.","tags":"datamanagement","title":"Study specification"},{"url":"datamanagement/conversion/","text":"The conversion of a study raw dataset to a BIDS compliant dataset relies on a proper specification . The outcome of such a conversion is again a datalad dataset. It contains a reference to the study dataset it was build from, but can be used and shared without access to the study dataset. Dataset creation First, create the to-be BIDS dataset pretty much the same way as you create a study dataset : datalad create [TARGET-DIR] If you don't provide a target dir, the dataset will be created in the current working directory. This will create an empty datalad dataset. To preconfigure it to be a BIDS dataset however, you need to run a dedicated setup procedure from within the dataset: cd [TARGET-DIR] datalad run-procedure setup_bids_dataset Reference needed input In order to achieve reproducibility and link the exact environment that was used for DICOM conversion run: datalad containers-add conversion \\ -u shub://mih/ohbm2018-training:heudiconv \\ --call-fmt 'singularity exec --bind {{pwd}} {img} {cmd}' Please note, that the recommended environment to be used will change soon, since we will provide a toolbox dataset suited for this platform and its usage in Magdeburg. Now the study dataset to be converted is needed. Install it as a subdataset sourcedata into the BIDS dataset: datalad install -d . -s [PATH TO STUDY DATASET] sourcedata Conversion Assuming that the study dataset comes with a proper study specification, you can now convert it by calling: datalad hirni-spec2bids --anonymize sourcedata/*/studyspec.json Several things are to be noted here. First, there is a switch --anonymize . This is optional and ensures that within the resulting BIDS dataset subjects are referred to only by their anon_subject ID according to the specification. There shouldn't be any hint on the original subject ID in the commit messages or paths in the new dataset. What is doesn't do, however, is defacing the images. This is an additional step to be taken, that requires the images to already be converted. Furthermore, you may notice that the call as shown above references not the study dataset to be converted but the specification files. This means, you don't need to convert the entire dataset at once. You can also convert a single acquisition instead. Dropping raw data Finally, you can uninstall the source dataset by running: datalad uninstall -d . -r --nocheck sourcedata This will leave you with just the BIDS dataset. It still contains a reference to the data it was derived from, but doesn't contain that data.","tags":"datamanagement","title":"Raw Data Conversion"},{"url":"datamanagement/demo/","text":"TODO : title and introductory description of what this is about* Creating a raw dataset First off, we need a study raw dataset to bundle all raw data in a structured way: datalad create my_raw_dataset cd my_raw_dataset datalad run-procedure setup_study_dataset TODO : Already start editing study metadata here? From within the dataset add our toolbox by calling: datalad install --dataset . --source https://github.com/psychoinformatics-de/cbbs-toolbox.git code/toolbox Acquiring data From the scanner, we'd usually get a tarball containing the DICOMs of an acquisition. For this demo we can get such a tarball via: wget https://github.com/datalad/example-dicom-functional/archive/half.tar.gz TODO: acquisition ID, since DICOM contains subject ID only, which would become the acquisition ID as well Now, as soon as we have an acquisition's data, we import it into the raw dataset to make sure, we get its metadata as well as the correct starting point for the data versioning. From within the my_raw_dataset directory we run: datalad hirni-import-dcm /path/to/downloaded/half.tar.gz TODO : Note on ReproIn Convention ( https://github.com/repronim/reproin#overall-workflow ) TODO : Import of additional data Editing the specification This step isn't actually required in case of this example. However, if there was a need to change the specification of the study (or a single acquisition), you can either edit the JSON files directly or use the WebUI: % datalad webapp --dataset . hirni Output ends with: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) Now this address can be opened in a browser and should look like this: Click on \"Edit acquisition metadata\" and select acquisition \"02\" to get to the editing mask for this acquisition: You can also edit more general information about the study, if you choose to edit study metadata: Conversion to BIDS In order to get a BIDS dataset from the raw dataset, create a new dataset and set it up to become a BIDS dataset: datalad create bids cd bids datalad run-procedure setup_bids_dataset Now, install input data as a subdataset: datalad install --dataset . --source ../study_ds sourcedata datalad install sourcedata/code/toolbox If the specification wasn't altered, the actual conversion is done by: datalad hirni-spec2bids sourcedata/02/studyspec.json Note, that this command takes a list of specification files (each expected to be an acquisition specification) and converts the respective acquisitions. If you want to convert a dataset with multiple acquisitions at once, just use: datalad hirni-spec2bids sourcedata/*/studyspec.json","tags":"datamanagement","title":"Demo Study"},{"url":"datamanagement/demo_scandb/","text":"DICOM datasets that have been imported into a study raw dataset can (additionally) be collected in scanner (or institution or lab) specific superdatasets. This allows for convenient record keeping of all relevant MR data acquisitions ever made in a given context. The example script at the bottom of this page shows how to bootstrap such a database. Such superdatasets are lightweight, as they do not contain actual imaging data, and can be queried using a flexible language. In the DICOM context it is often desired to limit the amount of metadata to whole datasets and their image series. This can be achieved using the following configuration, which only needs to be put into the top-most dataset, not every DICOM dataset: % cat .datalad/config [datalad \"dataset\"] id = 349bb81a-1afe-11e8-959f-a0369f7c647e [datalad \"search\"] index-autofield-documenttype = datasets default-mode = autofield With this setup the DataLad search command will automatically discover metadata for any contained image series, and build a search index that can be queried for values in one or more individual DICOM fields. This allows for a variety of useful queries. Example queries Report scans made on any male patients in a given time span: % datalad search dicom.Series.AcquisitionDate:'[20130410 TO 20140101]' dicom.Series.PatientSex:'M' search(ok): lin/7t/xx99_2022/dicoms (dataset) Report any scans for a particular subject ID: % datalad search 'xx99*' [INFO ] Query completed in 0.019682836998981657 sec. Reporting up to 20 top matches. search(ok): lin/7t/xx99_2022/dicoms (dataset) search(ok): lin/7t/xx99_2014/dicoms (dataset) search(ok): lin/7t/xx99_2015/dicoms (dataset) search(ok): lin/3t/xx99_0138/dicoms (dataset) search(ok): lin/3t/xx99_0139/dicoms (dataset) search(ok): lin/3t/xx99_0140/dicoms (dataset) action summary: search (ok: 6) For each search hit ALL available metadata is returned. This allows for sophisticated output formating. Here is an example that reports all studies a particular subject has participated in: % datalad -f '{metadata[dicom][Series][0][StudyDescription]}' search 'xx99*' | uniq [INFO] Query completed in 0.02244874399912078 sec. Reporting up to 20 top matches. Studies&#94;Michael_Hanke transrep2 Transrep2 Demo script to bootstrap a DICOM database from scan tarballs The following script shows how a bunch of DICOM tarballs from two different scanners can be imported into a DataLad superdataset for each scanner. Those two scanner datasets are than assembled into a joint superdataset for acquisition hardware of the institution. Metadata from any acquisition session can then be aggregated into this dataset, to track all acquisitions made on those devices, as well as to be able to query for individual scan sessions, DICOM series, or individual DICOM images (see above for query examples). # get the tools datalad install -s https://github.com/psychoinformatics-de/cbbs-imaging-container-import import datalad get import/cbbs-imaging.simg # create a super dataset that will have all acquisitions the 7T ever made singularity run import/cbbs-imaging.simg create 7t cd 7t # import a bunch of DICOM tarballs (simulates daily routine) singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/pandora/data/xx99/raw/dicom/xx99_2022.20130410.103515.930000.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2014.20130408.123933.087500.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2015.20130408.140515.147500.tar.gz # done for now cd .. # now the same for 3t singularity run import/cbbs-imaging.simg create 3t cd 3t # import a bunch of DICOM tarballs singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0138.20140425.121603.06.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0139.20140425.142752.07.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_visloc/mri/xx99_0140.20140425.155736.23.tar.gz # done cd .. # one dataset for the entire institute's scan (could in turn be part of one that also # includes other modalities/machines) # this first part only needs to be done once datalad create lin cd lin datalad install -d . -s ../7t datalad install -d . -s ../3t # this second part needs to be done everytime the metadata DB shall be updated # get the latest state of the scanner datasets (no heavy stuff is moved around) datalad update --merge -r # aggregate from the aggregated metadata datalad aggregate-metadata -r # ready to search","tags":"datamanagement","title":"Demo: DICOM DB"},{"url":"datamanagement/tools/","text":"DataLad Git WebUI","tags":"datamanagement","title":"Tools"},{"url":"datamanagement/tools/datalad/","text":"","tags":"tools","title":"Datalad"},{"url":"datamanagement/tools/webui/","text":"","tags":"tools","title":"WebUI"},{"url":"datamanagement/tools/git/","text":"Git enables you to track the changes made to files over time — specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt-get install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Setup Once Git is installed, configure it with your name and email address. This lets Git know who you are so that it can associate you with the commits you make. git config --global user.name \"John Doe\" git config --global user.email johndoe@example.com Basic Commands git init Tells git to enable tracking of changes that happen in this folder. git clone <url> | <user@server:path/to/repo.git> Makes a full copy of an existing git repository — all files, folders, changes, history, etc. git status Lists which files are in which state — if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made and select which will be staged . git commit Commits all the staged changes (done with git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Stages the file to be removed. After you commit, the file will be removed and no longer tracked. But the file does remain in the project history. git mv <file-from> <file-to> Moves/renames a file. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and will need to be installed ( apt-get install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching if you're still unsure about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","title":"Git"},{"url":"analysis/","text":"","tags":"pages","title":"Data Analysis"},{"url":"devices/","text":"The pages in this section contain information on data acquisition devices (to be) supported in the CBBS imaging platform.","tags":"pages","title":"Acquisition Hardware"},{"url":"devices/magnetom/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"7T Siemens Magnetom"},{"url":"devices/achieva/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Philips Achieva"},{"url":"devices/prisma/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Siemens Prisma"},{"url":"devices/skyra/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Skyra"},{"url":"devices/verio/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Verio"},{"url":"devices/eyelink/","text":"MR-compatible Eyelink 1000 (responsible: Stefan Pollmann). Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Eyelink 1000"},{"url":"devices/stadlerphysio/","text":"This is Jörg's physio recording box that can be used with the Siemens MR scanners. Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Physio box"},{"url":"containers/","text":"In order to faciliate the reproducibility of research computing, all data processing is performed in containerized computational environments. These environments can be archived alongside the results of the data processing algorithms they have performed, in order to generate a complete record of the processing history. Moreover, such computional environment can be tracked with version control systems to capture which particular results were produced with which software version, in case data processing pipelines have to be adjusted throughout the lifetime of a project. The CBBS imaging platform employs Singularity as the main workhorse for container-based data processing.","tags":"pages","title":"Computational Environments"},{"url":"containers/rawimport/","text":"This Singularity container contains all software necessary to perform raw data import for all supported data types, and for conversion into the common data structure. Development information Report issues","tags":"containers","title":"Raw data import"},{"url":"contributing/","text":"This project welcomes contributions from platform users and developers. The following list provides information on how to contribute to individual aspects of the imaging platform: Contribute to this documentation Contribute an implementation of a new analysis procedure In general, all documentation and source code are available under free and open source licences in various repositories on GitHub . Any contributions in the form of reported issues, or patches are much appreciated.","tags":"pages","title":"Contributing"},{"url":"contributing/docs/","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. If your proposed fix is limited to content, then you can probably skip setting up the build system. You can simply make your changes (content is written in reStructuredText ) and submit a Pull Request . The website is generated by Pelican and the template is written using Jinja2 . So, if your proposed changes are more widespread or change the template, you'll need to setup the build environment. Don't worry; it's easy. install pelican ( apt-get install pelican on Debian; pip install --user pelican on other OSs.) install beautifulsoup4 (again: apt-get install python-beautifulsoup or pip install --user beautifulsoup4 respectively), which is required by some pelican plugins we use run make devserver open a browser to http://127.0.0.1:8000 And that's it. Any changes you make will automatically trigger a rebuild.","tags":"contributing","title":"Documentation"}]}