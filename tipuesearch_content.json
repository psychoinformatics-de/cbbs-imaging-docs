{"pages":[{"url":"datamanagement/","text":"The fundamental concept of this platform is that data are first transformed into a standard layout using common file formats, so that all processing pipelines can expect a deterministic, well described data structure. This setup enables the development of data processing components that are agnostic of the peculiarities of individual studies, as long as all relevant aspects of a study are properly described. The necessary data conversion and layout are performed largely automatic. The procedures are tuned for data acquired in Magdeburg, but with minor extra effort data from other sources can be processed too. For each study the following steps will be performed: Initial study setup: create a study raw dataset Import a new acquisition (repeated as necessary): Import a DICOM dataset Import other data modalities Complete study description (if necessary): Identify data that were acquired simultaneously during an acquisition Associate stimulation and behavior logs with (MR) data acquisitions Identify data formats/converters for non-standard acquisitions (e.g. custom hardware) possibly use datalad-hirni's webUI to edit the specification accordingly Convert raw data into the common data structure Verify result of automatic conversion (automatic validation tools are available) Apply analysis procedure Detailed step-by-step demos are included in many section to help getting started with this platform.","tags":"pages","title":"Data Management"},{"url":"datamanagement/study_setup/","text":"Overview A study raw dataset contains all raw data and metadata on a study, across all acquisitions and participants. It is typically created just prior the first (pilot) acquisition. Its purpose is to record all data in their raw, unaltered form, as provided by the respective acquisition machinery. Moreover, it is used to bind data from multiple acquisition sources (e.g. MRI, eye tracker, ECG) together, to create a reliable record of what data were recorded at which point on what hardware. Importantly, this also includes the logs of any stimulation setup, or behavior responses that are typically generated by paradigm implementations provided by individual researchers. The study raw dataset should capture all information required to reproduce any subsequently performed data analyses. A study raw dataset is supposed to have a subdirectory for each acquisition. The name of this directory is the acquisition identifier as far as the tools described here are concerned. Each of those acquisition directories should contain all relevant data of an acquisition. Most notably they contain a subdataset dicoms and a specification file to curate required information for conversion . It is recommended to group other data according to its source and nature similarly (i.e. have a physio subdirectory alongside the dicoms subdirectory). The study raw dataset should also contain custom code needed for conversion in order to provide reproducibility. Preparation Data management relies on datalad and datalad-hirni respectively, which is an extension to datalad to provide additional functionality for this platform. Start off by installing datalad-hirni . Dataset creation To create a study dataset you just need to run: datalad rev-create [TARGET-DIR] If you don't provide a target dir, the dataset will be created in the current working directory. This will create an empty datalad dataset. To preconfigure it to be a study dataset however, you need to run a dedicated setup procedure from within the dataset: cd [TARGET-DIR] datalad run-procedure setup_hirni_dataset This command will do several things to make this a study dataset. Apart from setting some configurations like enabling the extraction of DICOM metadata, it will create a default README file, a dataset_description.json template file, an initial study specification file and it will install hirni's toolbox dataset as a subdataset of my_raw_dataset . Note, that by default the toolbox is installed from github. If you need to install from elsewhere, you can set the datalad.hirni.toolbox.url config to point to another URL prior to running setup_hirni_dataset . You can now use hirni's webUI to edit the study metadata and thereby fill the dataset_description.json file. This step can be postponed or repeated later on to complete it. There's no technical need to fill it out completely (or at all) at this point. It is recommended to do it as far as you can though. The dataset is now ready for importing acquisition data. Step-by-step demo This is a simple example showing how to create a study dataset with datalad-hirni and how to import data into such a dataset. The raw data we use for this demo is publicly available from two example repositories at github. For reference what this data is about, simply visit https://github.com/datalad/example-dicom-functional/ and https://github.com/datalad/example-dicom-structural/ in your browser. For all commands to work in the exact form shown here, create a directory for that demo first and switch into it: % mkdir demo % cd demo Creating a raw dataset First off, we need a study raw dataset to bundle all raw data in a structured way: % datalad rev-create my_raw_dataset % cd my_raw_dataset % datalad run-procedure setup_hirni_dataset The first command will create a datalad dataset with nothing special about it. The last, however, runs a hirni procedure, that will do several things to make this a study dataset. Apart from setting some configurations like enabling the extraction of DICOM metadata, it will create a default README file, a dataset_description.json template file, an initial study specification file and it will install hirni's toolbox dataset as a subdataset of my_raw_dataset . Note, that by default the toolbox is installed from github. If you need to install from elsewhere, you can set the datalad.hirni.toolbox.url config to point to another URL prior to running setup_hirni_dataset . It now should like this: % tree -L 2 . ├── code │ └── hirni-toolbox ├── dataset_description.json ├── README └── studyspec.json And from a datalad perspective like this: % datalad ls -r . [annex] master ✗ 2019-02-28/12:37:01 ✓ code/hirni-toolbox [annex] master ✗ 2019-02-27/21:23:59 ✓ We now have an initial study dataset and should start by editing the study metadata, which is stored in dataset_description.json . For convenience when doing this manually we can use hirni's web UI: % datalad webapp --dataset . hirni The output following this command should end reading Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) . Now this address can be opened in a browser and should look like this: Choose \"Edit study metadata\" (we have no acquisition yet) to get to this form: It's not required to fill this at this point (technically it's not required to be filled at any point), but generally recommended to record whatever information you have ASAP into that dataset. Its recorded history is just as useful as you allow it to be. Acquiring data Now, we want the actual data. To import a DICOM tarball into the study dataset, there is a dedicated hirni command hirni-import-dcm . This will add the DICOMS to our dataset, extract metadata from their headers and derive a specification for each series it finds in those DICOM files. A hirni study dataset is supposed to put all data of each acquisition into a dedicated subdirectory, which also contains a specification file for that acquisition. We can give the command a name for such an acquisition or let it try to derive one from what it finds in the DICOM headers. Everything that is automatically concluded from the metadata can be overwritten by options to that command, of course. Something that can't automatically be derived, of course, are anonymized subject identifiers. This association will be needed for anonymized conversion. You can add those IDs later, of course, but we can do it right from the start via the option --anon-subject . datalad hirni-import-dcm can import such tarballs either from a local path or an URL. For this demo we use the above mentioned example data available from github: % datalad hirni-import-dcm --anon-subject 001 https://github.com/datalad/example-dicom-structural/archive/master.tar.gz acq1 This should create a new acquisition directory acq1 , containing a studyspec.json and a subdataset dicoms . Note, that this subdataset contains the original tarball itself (in a hidden way) and the extracted DICOMS. As long as we don't need to operate on the DICOM files, we don't really them to be there. We can throw their content away by calling: % datalad drop acq1/dicoms/* This should result in the DICOM files having no content. We can get them again any time via datalad get acq1/dicoms/* . Import the second acquisition the same way: % datalad hirni-import-dcm --anon-subject 001 https://github.com/datalad/example-dicom-functional/archive/master.tar.gz acq2 Note, that this imports and extracts metadata from about 6000 DICOM files. It will take a few minutes. This time we have something else to import for that acquisition: the events file. Generally, you can add arbitrary files to the dataset. Protocols, logfiles, physiological data, code - it is meant to bundle all raw data of study. The functional data already provides an events.tsv file and therefore we can find it already in the dicoms subdataset we just created. Since such a file is usually not included in a DICOM tarball you'd start with, lets pretend it's not actually in that archive and import it separately again. We use git annex addurl to retrieve that file and then save the new state of our dataset by calling datalad rev-save : % git annex addurl https://github.com/datalad/example-dicom-functional/raw/master/events.tsv --file acq2/events.tsv % datalad rev-save --message \"Added stimulation protocol for acquisition 2\" NOTE: The calls to git annex addurl and datalad rev-save currently replace a single call to datalad download-url due to a bug in that command. Please note, that the choice where exactly to put such a file within an acquisition directory is entirely up to you. datalad-hirni doesn't expect any particular structure within an acquisition. As long as the specification files are correctly referencing the locations of the data, everything is fine. Now, for a later conversion there is no general conversion rule for tsv files. We need to tell the system what it is supposed to do with that file (if anything) on conversion. For that, we add a specification for that file using hirni-spec4anything . This command allows to add (or replace) a specification for arbitrary things. By default it will generate a specification that already \"inherits\" everything, that is unambiguously uniform in the existing specifications of that acquisition. That means, if our automatically created specification for the functional DICOMs managed to derive all required BIDS terms (in this case it's about \"subject\", \"task\" and \"run\") and their values for the dicomseries, spec4anything will use that as well for the new specification (except we overrule this). So, all we need to do here, is to specify a conversion routine. For correct BIDS conversion we only need to copy that file to its correct location. Such a \"copy-converter\" is provided by the toolbox we have installed at the beginning. Editing or adding such a specification is again possible via the webUI. For the purpose of this demo, however, we will this time use the command line to show how that looks like: % datalad hirni-spec4anything acq2/events.tsv --properties '{\"procedures\": {\"procedure-name\": \"copy-converter\", \"procedure-call\": \"bash {script} {{location}} {ds}/sub-{{bids-subject}}/func/sub-{{bids-subject}}_task-{{bids-task}}_run-{{bids-run}}_events.tsv\"}, \"type\": \"events_file\"}' What we pass here into the properties option is a JSON string. This is the underlying structure of what you can see in the webUI. The necessary quoting/escaping at the command line is admittedly not always easy for manual editing. Note, that instead of such a string you can also pass a path to JSON file. (and more generally: All of datalad and the datalad-hirni extension is accessible via a Python API as well) For a more extensive description of the specification (and therefore those properties ) see the specification page . If you ran all the commands in this demo the exact same way as posted, your dataset should now look exactly like this: https://github.com/psychoinformatics-de/hirni-demo For comparison you can examine it on github or install it locally to have a closer look via: % cd .. % datalad install -s https://github.com/psychoinformatics-de/hirni-demo --recursive We now bound all information on that study and its acquisitions in its native, absolutely unmodified form together in a dataset that can now serve as a starting point for any kind of processing. This dataset is much less likely to suffer from software bugs than a ready-to-analyze dataset with NIfTIs etc, but the software stack that actually touched the data files is minimal.","tags":"datamanagement","title":"Initial Study Setup"},{"url":"datamanagement/study_specification/","text":"The study specification consists of JSON files in the study raw dataset and provides metadata needed for conversion. The tools provided by datalad-hirni aim to help to (automatically) create and curate those files and convert a study raw dataset (or single acquisition) based on them. Each acquisition in a study raw dataset contains such a file. Its default name is studyspec.json directly underneath the acquisition's directory. It's a JSON-stream consisting of one dictionary per line. We are referring to those dictionaries as snippets . For any data entity in an acquisition (at least any that is to be converted) there should be such a snippet. Those snippets are automatically created when you import a DICOM tarball or import other data into your study raw dataset. However, the automatic creation may be incomplete or even incorrect, depending on your needs. Therefore those specifications need to be edited before converting the dataset. Since those files are simple JSON files, you can change them programmatically, edit them manually with any kind of editor you like or use datalad-hirni's webUI to do so. The specification snippets have keys, that are meant to be edited and some special keys, that are not supposed to be changed, but only to automatically be set on creation. The editable ones have values that are again a dictionary with two keys: value and approved , where approved is a flag signalling whether or not the value is just an automatic guess, that needs to either be confirmed or changed. This is meant to help curation. Generally, a snippet for a dicom series looks like this: {\"location\":\"dicoms\", \"type\":\"dicomseries\", \"uid\":\"1.3.10.2.1102.5.2.34.18716.201347010933155397510580.0.0.0\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_modality\":{\"approved\":false,\"value\":\"bold\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"001\"}, \"description\":{\"approved\":true,\"value\":\"MoCoSeries_DiCo\"}, \"id\":{\"approved\":false,\"value\":9}, \"procedures\":[{\"procedure-name\": {\"approved\": false, \"value\": \"\"}, \"procedure-call\": {\"approved\": false, \"value\": \"\"}, \"on-anonymize\": {\"approved\": false, \"value\": false} }] \"tags\":[] } Note, that the first five entries are aforementioned non-editable fields. They are created by the import routine for DICOMs and supposed to not be changed. \"location\", \"type\", \"dataset_id\", \"dataset_refcommit\" are expected in every snippet independent on the type of data this specification is about. \"location\" is a path to the actual data relative to the specification file containing this very snippet. It might refer to a single file or a directory. \"comment\" is a field with no implications in how this snippet or the data it is about is dealt with. It is meant to provide a place to put notes regarding the data or TODOs or whatever you might need in order to finish creating the study raw dataset and preparing conversion. All keys starting with bids_ are referring to the terms used in the BIDS standard and are used for conversion. Many of those are optional and therefore can have a value of null or not appear in the specification at all. In this example the values were automatically derived from DICOM metadata (note, that they are not yet \"approved\"), most importantly from the DICOM field ProtocolName . For conversion of DICOMS currently supported BIDS keys are: 'bids_session' 'bids_task' 'bids_run' 'bids_modality' 'bids_acquisition' 'bids_scan' 'bids_contrast_enhancement' 'bids_reconstruction_algorithm' 'bids_echo' 'bids_direction' For details on their meaning please refer to the BIDS standard . The value of \"description\" comes from DICOM's SeriesDescription and \"id\" is prefilled with the value of SeriesNumber . It's worth noting, that this \"id\" has no technical meaning for datalad-hirni's commands. It is meant to provide you with a human-readable identification of this DICOM series to ease editing of the snippet. \"procedures\" defines a list of datalad-procedures to be executed during conversion. Besides its name such a procedure definition provides the option to overwrite the way it is called (\"procedure-call\") by passing an alternative format string and a flag to signal whether or not it is to be called only if the conversion with datalad hirni-spec2bids is called with the option --anonymize , which can be used to apply a defacing routine for example. An example for such a format string can be found within the study dataset demo . Such a format string is first and foremost meant to provide arguments to the call for a procedure and the mechanism as provided by datalad itself (see datalad-procedures for reference) is enhanced by datalad-hirni. This enhancements makes other fields of the specification snippet available via additional placeholders. Enclose any key of the snippet in double curly brackets and it will be replaced by the value of that key when the procedure is called. Procedures are available through hirni's toolbox dataset, but can also be provided by yourself. Any datalad-procedure is valid for this specification. With respect to the example snippet above, note that there is no conversion procedure defined for the dicomseries. This is because of the default routine hirni uses. This is a containerized version of heudiconv provided by the toolbox. The respective procedure is called hirni-dicom-converter and is only called once per acquisition. This will convert all the dicomseries within that acquisition. Hence any procedure you add to a particular dicomseries will be called after the conversion and usually there shouldn't be a need to do so. You can however exclude a dicomseries from conversion by adding the tag hirni-dicom-converter-ignore to its specification snippet. The conversion routine for the dicomseries is specified in an additional snippet (created automatically during import ) of type dicomseries:all . Similarly, a snippet for a physio file may look like this: {\"location\":\"physio/df37_200Hz_1.txt\", \"type\":\"physio_file\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"oneback\"}, \"sampling-frequency\": {\"approved\": true, \"value\": \"200Hz\"}, \"procedures\":[{\"procedure-name\":{\"approved\":true,\"value\":\"hirni-physiobox-converter\"}}], } Something like this is typically created when importing other data than DICOM files by calling datalad hirni-spec4anything .","tags":"datamanagement","title":"Study Metadata"},{"url":"datamanagement/import_dicoms/","text":"datalad-hirni provides a command to import a DICOM tarball for an acquisition into a study dataset . The command to be used for that is: datalad hirni-import-dcm [TARBALL] [ACQUISITION ID] from within your study dataset. This will result in a subdirectory ACQUISITION ID in your study dataset and a subdataset dicoms beneath with the DICOMs in it, that provides the DICOM metadata for easy access via datalad . In addition a prefilled specification for each series in the tarball is created and stored in the acquisition's specification file . If you don't provide an acquisition identifier, a name like xx99_0123 will be determined from the DICOM metadata. By default this is the value of field PatientID . However, there are special rules in place (and to further be developed) to be applied based on the scanner the data was acquired with. This mechanism allows for different rules per scanner, institution or any other category that can be identified by the DICOM metadata. Use --subject to provide a subject ID. Otherwise the import routine will try to derive the subject ID from the DICOM metadata. A typical case would be the aforementioned acquisition xx99_0123 with a corresponding subject ID xx99 . Optionally, you can use --anon-subject to additionally provide an anonymized subject ID. When converting the dataset to BIDS, the switch --anonymize will then determine which subject ID to use for the converted dataset. Generally, the option --properties allows to add and/or overwrite the specification to be created for this acquisition by passing either a JSON string or a path to a JSON file. Thereby you can assign a task label for example, if it can not be derived from the DICOM metadata by the rules inplace: datalad hirni-import-dcm --subject xx99 --anon-subject 007 \\ --properties '{\"bids_task\": \"dofancystuff\", \\ \"comment\": \"something unusual happened during this acquisition\"}' \\ /path/to/tarball Query DICOM metadata DICOM datasets that have been imported into a study raw dataset can (additionally) be collected in scanner (or institution or lab) specific superdatasets. This allows for convenient record keeping of all relevant MR data acquisitions ever made in a given context. The example script at the bottom of this page shows how to bootstrap such a database. Such superdatasets are lightweight, as they do not contain actual imaging data, and can be queried using a flexible language. In the DICOM context it is often desired to limit the amount of metadata to whole datasets and their image series. This can be achieved using the following configuration, which only needs to be put into the top-most dataset, not every DICOM dataset: % cat .datalad/config [datalad \"dataset\"] id = 349bb81a-1afe-11e8-959f-a0369f7c647e [datalad \"search\"] index-autofield-documenttype = datasets default-mode = autofield With this setup the DataLad search command will automatically discover metadata for any contained image series, and build a search index that can be queried for values in one or more individual DICOM fields. This allows for a variety of useful queries. Example queries Report scans made on any male patients in a given time span: % datalad search dicom.Series.AcquisitionDate:'[20130410 TO 20140101]' dicom.Series.PatientSex:'M' search(ok): lin/7t/xx99_2022/dicoms (dataset) Report any scans for a particular subject ID: % datalad search 'xx99*' [INFO ] Query completed in 0.019682836998981657 sec. Reporting up to 20 top matches. search(ok): lin/7t/xx99_2022/dicoms (dataset) search(ok): lin/7t/xx99_2014/dicoms (dataset) search(ok): lin/7t/xx99_2015/dicoms (dataset) search(ok): lin/3t/xx99_0138/dicoms (dataset) search(ok): lin/3t/xx99_0139/dicoms (dataset) search(ok): lin/3t/xx99_0140/dicoms (dataset) action summary: search (ok: 6) For each search hit ALL available metadata is returned. This allows for sophisticated output formating. Here is an example that reports all studies a particular subject has participated in: % datalad -f '{metadata[dicom][Series][0][StudyDescription]}' search 'xx99*' | uniq [INFO] Query completed in 0.02244874399912078 sec. Reporting up to 20 top matches. Studies&#94;Michael_Hanke transrep2 Transrep2 Demo script to bootstrap a DICOM database from scan tarballs The following script shows how a bunch of DICOM tarballs from two different scanners can be imported into a DataLad superdataset for each scanner. Those two scanner datasets are than assembled into a joint superdataset for acquisition hardware of the institution. Metadata from any acquisition session can then be aggregated into this dataset, to track all acquisitions made on those devices, as well as to be able to query for individual scan sessions, DICOM series, or individual DICOM images (see above for query examples). # create a super dataset that will have all acquisitions the 7T ever made datalad rev-create 7t cd 7t datalad run-procedure setup_hirni_dataset # import a bunch of DICOM tarballs (simulates daily routine) datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/pandora/data/xx99/raw/dicom/xx99_2022.20130410.103515.930000.tar.gz datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2014.20130408.123933.087500.tar.gz datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/7T_ad/data/xx99/raw/dicom/xx99_2015.20130408.140515.147500.tar.gz # done for now cd .. # now the same for 3t datalad rev-create 3t cd 3t # import a bunch of DICOM tarballs datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0138.20140425.121603.06.tar.gz datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/xx99_0139.20140425.142752.07.tar.gz datalad hirni-import-dcm \\ /home/data/psyinf/forrest_gump/3T_visloc/mri/xx99_0140.20140425.155736.23.tar.gz # done cd .. # one dataset for the entire institute's scan (could in turn be part of one that also # includes other modalities/machines) # this first part only needs to be done once datalad rev-create lin cd lin datalad install -d . -s ../7t datalad install -d . -s ../3t # this second part needs to be done everytime the metadata DB shall be updated # get the latest state of the scanner datasets (no heavy stuff is moved around) datalad update --merge -r # aggregate from the aggregated metadata datalad aggregate-metadata -r # ready to search","tags":"datamanagement","title":"DICOM Data"},{"url":"datamanagement/import_other/","text":"Apart from DICOM files, you can (and should) add any additional data into a study dataset. Information corresponding to any of the MR data acquisitions must be added into the respective directory. This can be stimulation logs, behavioral response logs, or other simultaneously acquired data. To simply add the files to the dataset, copy or move them to the appropriate location and use datalad rev-save to make them part of the dataset. Have a look at the study dataset demo for an example. However, in order to include the data in the conversion later on, you need to create a specification for it. This is what datalad hirni-spec4anything is for. This will create a snippet for the data component, containing the mandatory pieces and lets you specify all properties of the specification needed for conversion via the same option --properties , that is available when importing DICOM archives . Let's say you added a file physio/df37_200Hz_1.txt to an acquisition. To create a proper specification snippet for that file, from within the acquisition directory you'd call: datalad hirni-spec4anything physio/df37_200Hz_1.txt \\ --properties \"{'type': 'physio_file', \\ 'bids_run': '01', \\ 'procedures': [{'procedure-name': 'hirni-physiobox-converter'}], \\ 'sampling-frequency': '200Hz'}\" Note, that the specified type is an arbitrary label you can choose to distinguish different kinds of files. It can be used to run a conversion on that type only or to ease programmatic curation of specifications. datalad hirni-spec4anything will also prefill the specification with values that are unambiguous throughout the already existing snippets in the respective specification file. Hence, if you already have imported the corresponding DICOMs, things like the subject ID or a possible task label that are the same for all the specification snippets derived from the DICOM metadata will be filled into your snippet for that physio file. You can overwrite them of course, by passing new values via that --properties option, which takes precedence over the defaults. The result will be a snippet in the acquisitions specification file as shown on the specification page : {\"location\":\"physio/df37_200Hz_1.txt\", \"type\":\"physio_file\", \"dataset_id\":\"42bc01a0-a6c3-11e8-9a5b-a0389fb56dc0\", \"dataset_refcommit\":\"a771150f0e15f309212bc83186d893c65f731d9c\", \"comment\":{\"approved\":false,\"value\":\"\"}, \"subject\":{\"approved\":false,\"value\":\"df37\"}, \"anon_subject\":{\"approved\":false,\"value\":\"002\"}, \"bids_run\":{\"approved\":false,\"value\":\"01\"}, \"bids_session\":{\"approved\":false,\"value\":null}, \"bids_task\":{\"approved\":true,\"value\":\"oneback\"}, \"sampling-frequency\": {\"approved\": true, \"value\": \"200Hz\"}, \"procedures\":[{\"procedure-name\":{\"approved\":true,\"value\":\"hirni-physiobox-converter\"}}], } Note, that the keys \"subject\", \"anon_subject\", \"bids_task\" were not specified in the call but still created for this snippet (assuming they can be concluded as described above). If there was only one run in the existing data there would also be no need to specify it in the call to spec4anything . The field procedures defines a list of datalad-procedures to be called on conversion. They can refer to procedures provided by datalad-hirni's toolbox or any other procedure you defined. For details on how to this, please refer to datalad run-procedure --help . Note, that the procedure in this example ( hirni-physiobox-converter ) is provided by the toolbox and expects a field sampling-frequency in the specification snippet. You might need different values and/or keys depending on your data. Generally, you can add arbitrary fields to the specification whether or not they are currently needed by any procedure. Thereby you can enhance metadata for everything within your study dataset in a machine-readable shape, that potentially can be processed later on.","tags":"datamanagement","title":"Non-DICOM Data"},{"url":"datamanagement/conversion/","text":"The conversion of a study raw dataset to a BIDS compliant dataset relies on a proper specification . The outcome of such a conversion is again a datalad dataset. It contains a reference to the study dataset it was build from, but can be used and shared without access to the study dataset. Dataset creation First, create the to-be BIDS dataset pretty much the same way as you create a study dataset : datalad rev-create [TARGET-DIR] If you don't provide a target dir, the dataset will be created in the current working directory. This will create an empty datalad dataset. To preconfigure it to be a BIDS dataset however, you need to run a dedicated setup procedure from within the dataset: cd [TARGET-DIR] datalad run-procedure setup_bids_dataset Reference needed input Now the study dataset to be converted is needed. Install it as a subdataset sourcedata into the BIDS dataset: datalad install -d . -s [PATH TO STUDY DATASET] sourcedata Note, that we are assuming you created your study dataset according to the setup page or the accompanying demo . That is, it has the toolbox installed as a subdataset. Conversion Assuming that the study dataset comes with a proper study specification, you can now convert it by calling: datalad hirni-spec2bids --anonymize sourcedata/studyspec.json sourcedata/*/studyspec.json Several things are to be noted here. First, there is a switch --anonymize . This is optional and ensures that within the resulting BIDS dataset subjects are referred to only by their anon_subject ID according to the specification. There shouldn't be any hint on the original subject ID in the commit messages or paths in the new dataset. By default this should also run a defacing routine, that should be specified in an acquisition's specification file within a snippet of type dicomseries:all . You can change this by editing this specification. Furthermore, you may notice that the call as shown above references not the study dataset to be converted but the specification files. This means, you don't need to convert the entire dataset at once. You can also convert a single acquisition instead. In fact, you can even have several specification files per acquisition and run a conversion based on a single file. Further limitation is available via the option --only-type , which allows to convert only snippets of that particular type. Dropping raw data Finally, you can uninstall the source dataset by running: datalad uninstall -d . -r --nocheck sourcedata This will leave you with just the BIDS dataset. It still contains a reference to the data it was derived from, but doesn't contain that data. Demo: Conversion to BIDS This demo shows how to convert a hirni study dataset into a BIDS compliant dataset. The study dataset we use is the one created by the study dataset demo . We will use a published version of that dataset available from github, but you can also build it yourself by following said demo and use that one. BIDS Dataset The idea is to create a new dataset, that will become the BIDS Dataset and reference our study dataset - that bundles all the raw data - by making it a subdataset of the derived one. Please note, that this does NOT mean, that the new BIDS dataset contains the raw data. It just references it and thereby creates a fully reproducible history record of how it came to be. The study dataset does NOT need to be shared if you want to share the BIDS dataset. Rather it is possible to trace everything back to the original raw data for everyone who has the BIDS dataset IF he also has access/permission to get that subdataset. In order to get our to-be BIDS dataset from the raw dataset, we create a new dataset and run the setup_bids_dataset procedure to configure it: % datalad rev-create demo_bids % cd demo_bids % datalad run-procedure setup_bids_dataset Now we install our study dataset as a subdataset into our new dataset at its subdirectory sourcedata . By that, we reference the exact state of our study dataset at the moment of installation. While this may create some data duplication, please note several things: First, the new subdataset doesn't need to hold all of the actual content of the study dataset's files (although it can retrieve it, it doesn't by default during installation). Rather it's about referencing the input data (including the code and environments in hirni's toolbox) at their exact version to achieve full reproducibility. We can thereby track the converted data back to the raw data and the exact conversion routine that brought it into existence. Second, this subdataset can later be removed by datalad uninstall , freeing the space on the filesystem while keeping the reference: % datalad install --dataset . --source https://github.com/psychoinformatics-de/hirni-demo sourcedata --recursive Note, that if you want to use a local study dataset (i.e. created yourself via the study dataset demo) you can simply replace that URL with the path to your local one. The actual conversion is based on the specification files in the study dataset. You can convert a single one of them (meaning: Everything such a file specifies) or an arbitrary number, including everything at once, of course. Lets first convert the study level specification and second all the acquisitions by the following call: % datalad hirni-spec2bids --anonymize sourcedata/studyspec.json sourcedata/*/studyspec.json The anonymize switch will cause the command to use the anonymized subject identifiers and encode all records of where exactly the data came from into hidden sidecar files, that can tha be excluded from publishing/sharing this dataset. datalad hirni-spec2bids will run datalad procedures on the raw data as specified in the specification files (remember for example that we set a procedure \"copy-converter\" for our events.tsv file). Those procedures are customizable. The defaults we are using here, come from hirni's toolbox dataset. The default procedure to convert the DICOM files uses a containerized converter. It will NOT use, what you happen to have locally, but this defined and in the datasets referenced environment to do the conversion. This requires a download of that container (happens automatically) and enables the reproducibility of this routine, since the exact environment the conversion was ran in will be recorded in the dataset's history. In addition, this will cause datalad to retrieve the actual data of the study subdataset in sourcedata . Remember that you can datalad uninstall that subdataset after conversion or use datalad drop to throw away its copy of particular files. If you use the BIDS-Validator ( https://bids-standard.github.io/bids-validator/ ) to check the resulting dataset, there should be an error message, though. This is because our events.tsv file references stimuli files, we don't actually have available to add to the dataset. For the purpose of this demo, this should be fine. Other than that, we have a valid BIDS dataset now, that can be used with BIDS-Apps or any kind of software that is able to deal with this standard. Since we have the raw data in a subdataset, we can aggregate DICOM metadata from it into the BIDS dataset, which would be available even when the study dataset was uninstalled from the BIDS dataset. If we keep using datalad-run / datalad containers-run for any processing to follow (as hirni internally does), we are able to trace back the genesis and evolution of each file to the raw data, the exact code and the environments it ran in to alter this file or bring it into its existence.","tags":"datamanagement","title":"BIDS Conversion"},{"url":"analysis/demo_reproducibility/","text":"This demo shows how to use datalad and datalad-hirni datasets to perform and record reproducible analyses. We will again use the study dataset as created by the respective demo to provide the raw data. Prepare the Data for Analysis Before analyzing imaging data, we typically have to convert them from their original DICOM format into NIfTI files. We gain a lot here by adopting the BIDS standard. Up front, it saves us the effort of creating an ad-hoc directory structure. But more importantly, by structuring our data in a standard way (and an increasingly common one), it opens up possibilities for us to easily feed our dataset into existing analysis pipelines and tools. For the purpose of this demo, we will simply list the commands needed to get a BIDS dataset from the study dataset. For reference: This is the exact same thing we do in the conversion demo % datalad rev-create localizer_scans % cd localizer_scans % datalad run-procedure setup_bids_dataset % datalad install --dataset . --source https://github.com/psychoinformatics-de/hirni-demo sourcedata --recursive % datalad hirni-spec2bids --anonymize sourcedata/studyspec.json sourcedata/*/studyspec.json We should now have a BIDS dataset looking like this: % tree -L 2 . ├── dataset_description.json ├── participants.tsv -> .git/annex/objects/KF/5x/MD5E-s50--12d3834067b61899d74aad5d48fd5520.tsv/MD5E-s50--12d3834067b61899d74aad5d48fd5520.tsv ├── README ├── sourcedata │ ├── acq1 │ ├── acq2 │ ├── code │ ├── dataset_description.json │ ├── README │ └── studyspec.json ├── sub-001 │ ├── anat │ ├── func │ └── sub-001_scans.tsv -> ../.git/annex/objects/8v/Gj/MD5E-s179--2e78ce543c5bcc8f0b462b7c9b334ad2.tsv/MD5E-s179--2e78ce543c5bcc8f0b462b7c9b334ad2.tsv └── task-oneback_bold.json -> .git/annex/objects/3J/JW/MD5E-s1452--62951cfb0b855bbcc3fce91598cbb40b.json/MD5E-s1452--62951cfb0b855bbcc3fce91598cbb40b.json % datalad subdatasets -r subdataset(ok): sourcedata (dataset) subdataset(ok): sourcedata/acq1/dicoms (dataset) subdataset(ok): sourcedata/acq2/dicoms (dataset) subdataset(ok): sourcedata/code/hirni-toolbox (dataset) action summary: subdataset (ok: 4) We are now done with the data preparation. We have the skeleton of a BIDS-compliant dataset that contains all data in the right format and using the correct file names. In addition, the computational environment used to perform the DICOM conversion is tracked, as well as a separate dataset with the input DICOM data. This means we can trace every single file in this dataset back to its origin, including the commands and inputs used to create it. This dataset is now ready. It can be archived and used as input for one or more analyses of any kind. Let's leave the dataset directory now: % cd .. A Reproducible GLM Demo Analysis With our raw data prepared in BIDS format, we can now conduct an analysis. We will implement a very basic first-level GLM analysis using FSL that runs in just a few minutes. We will follow the same principles that we already applied when we prepared the localizer_scans dataset: the complete capture of all inputs, computational environments, code, and outputs. Importantly, we will conduct our analysis in a new dataset. The raw localizer_scans dataset is suitable for many different analysis that can all use that dataset as input. In order to avoid wasteful duplication and to improve the modularity of our data structures, we will merely use the localizer_scans dataset as an input, but we will not modify it in any way. We will simply link it in a new analysis dataset the same way we did during conversion with the raw data: % datalad create glm_analysis % cd glm_analysis Following the same logic and commands as before, we will add the localizer_scans dataset as a subdataset of the new glm_analysis dataset to enable comprehensive tracking of all input data within the analysis dataset: % datalad install --dataset . --source ../localizer_scans inputs/rawdata Regarding the layout of this analysis dataset, we unfortunately cannot yet rely on automatic tools and a comprehensive standard (but such guidelines are actively being worked on). However, DataLad nevertheless aids efforts to bring order to the chaos. Anyone can develop their own ideas on how a dataset should be structured and implement these concepts in dataset procedures that can be executed using the datalad run-procedure command. Here we are going to adopt the YODA principles: a set of simple rules on how to structure analysis dataset. But here, the only relevant aspect is that we want to keep all analysis scripts in the code/ subdirectory of this dataset. We can get a readily configured dataset by running the YODA setup procedure: % datalad run-procedure setup_yoda_dataset Before we can fire up FSL for our GLM analysis, we need two pieces of custom code: a small script that can convert BIDS events.tsv files into the EV3 format that FSL can understand, available at https://raw.githubusercontent.com/myyoda/ohbm2018-training/master/section23/scripts/events2ev3.sh an FSL analysis configuration template script available at https://raw.githubusercontent.com/myyoda/ohbm2018-training/master/section23/scripts/ffa_design.fsf Any custom code needs to be tracked if we want to achieve a complete record of how an analysis was conducted. Hence we will store those scripts in our analysis dataset. We use the datalad download-url command to download the scripts and include them in the analysis dataset: % datalad download-url --path code \\ https://raw.githubusercontent.com/myyoda/ohbm2018-training/master/section23/scripts/events2ev3.sh \\ https://raw.githubusercontent.com/myyoda/ohbm2018-training/master/section23/scripts/ffa_design.fsf Note, that the commit message shows the URL where each script has been downloaded from: % git log At this point, our analysis dataset contains all of the required inputs. We only have to run our custom code to produce the inputs in the format that FSL expects. First, let's convert the events.tsv file into EV3 format files. We use the datalad run command to execute the script at code/events2ev3.sh. It requires the name of the output directory (use sub-001) and the location of the BIDS events.tsv file to be converted. The --input and --output options are used to let DataLad automatically manage these files for you. Important: The subdataset does not actually have the content for the events.tsv file yet. If you use --input correctly, DataLad will obtain the file content for you automatically. Check the output carefully, the script is written in a sloppy way that will produce some output even when things go wrong. Each generated file must have three numbers per line: % datalad run -m 'Build FSL EV3 design files' \\ --input inputs/rawdata/sub-001/func/sub-001_task-oneback_run-01_events.tsv \\ --output 'sub-001/onsets' \\ bash code/events2ev3.sh sub-001 {inputs} Now we're ready for FSL! And since FSL is certainly not a simple, system program, we will use it in a container and add that container to this analysis dataset. A ready-made container with FSL (~260 MB) is available from shub://ReproNim/ohbm2018-training:fsln. Use the datalad containers-add command to add this container under the name fsl. Then use the datalad containers-list command to verify that everything worked: % datalad containers-add fsl --url shub://ReproNim/ohbm2018-training:fsln % datalad containers-list With this we have completed the analysis setup. At such a milestone it can be useful to label the state of a dataset that can be referred to later on. Let's add the label ready4analysis here: % datalad save --version-tag ready4analysis All we have left is to configure the desired first-level GLM analysis with FSL. The following command will create a working configuration from the template we stored in code/ . It uses the arcane, yet powerful sed editor. We will again use datalad run to invoke our command so that we store in the history how this template was generated (so that we may audit, alter, or regenerate this file in the future — fearlessly): % datalad run \\ -m \"FSL FEAT analysis config script\" \\ --output sub-001/1stlvl_design.fsf \\ bash -c 'sed -e \"s,##BASEPATH##,{pwd},g\" -e \"s,##SUB##,sub-001,g\" \\ code/ffa_design.fsf > {outputs}' The command that we will run now in order to compute the analysis results is a simple feat sub-001/1stlvl_design.fsf. However, in order to achieve the most reproducible and most portable execution, we should tell the datalad containers-run command what the inputs and outputs are. DataLad will then be able to obtain the required NIfTI time series file from the localizer_scans raw subdataset. The following command takes around 5 minutes to complete on an average system: % datalad containers-run --container-name fsl -m \"sub-001 1st-level GLM\" \\ --input sub-001/1stlvl_design.fsf \\ --input sub-001/onsets \\ --input inputs/rawdata/sub-001/func/sub-001_task-oneback_run-01_bold.nii.gz \\ --output sub-001/1stlvl_glm.feat \\ fsl5.0-feat '{inputs[0]}' Once this command finishes, DataLad will have captured the entire FSL output, and the dataset will contain a complete record all the way from the input BIDS dataset to the GLM results (which, by the way, performed an FFA localization on a real BOLD imaging dataset, take a look!). The BIDS subdataset in turn has a complete record of all processing down from the raw DICOMs onwards. Get Ready for the Afterlife Once a study is complete and published it is important to archive data and results, for example, to be able to respond to inquiries from readers of an associated publication. The modularity of the study units makes this straightforward and avoid needless duplication. We now that the raw data for this GLM analysis is tracked in its own dataset (localizer_scans) that only needs to be archived once, regardless of how many analyses use it as input. This means that we can \"throw away\" this subdataset copy within this analysis dataset. DataLad can re-obtain the correct version at any point in the future, as long as the recorded location remains accessible. We can use the datalad diff command and git log to verify that the subdataset is in the same state as when it was initially added. Then use datalad uninstall to delete it: % datalad diff -- inputs/rawdata % git log -- inputs/rawdata % datalad uninstall --dataset . inputs/rawdata --recursive Before we archive these analysis results, we can go one step further and verify their computational reproducibility. DataLad provides a rerun command that is capable of \"replaying\" any recorded command. The following command we re-execute the FSL analysis (the command that was recorded since we tagged the dataset as \"ready4analysis\"). It will record the recomputed results in a separate Git branch named \"verify\" of the dataset. We can then automatically compare these new results to the original ones in the \"master\" branch. We will see that all outputs can be reproduced in bit-identical form. The only changes are observed in log files that contain volatile information, such as time steps: # rerun FSL analysis from scratch (~5 min) % datalad rerun --branch verify --onto ready4analysis --since ready4analysis % # check that we are now on the new `verify` branch % git branch % # compare which files have changes with respect to the original results % git diff master --stat % # switch back to the master branch and remove the `verify` branch % git checkout master % git branch -D verify So, hopefully we've shown that: we can implement a complete imaging study using DataLad datasets to represent units of data processing each unit comprehensively captures all inputs and data processing leading up to it this comprehensive capture facilitates re-use of units, and enables computational reproducibility carefully validated intermediate results (captured as a DataLad dataset) are a candidate for publication with minimal additional effort","tags":"analysis","title":"Demo: A Reproducible GLM Analysis"},{"url":"datamanagement/tools/","text":"Additional information on data management tools employed by the CBBS imaging platform is available from the pages linked below: DataLad Git WebUI Hirni-Toolbox","tags":"datamanagement","title":"Tools"},{"url":"datamanagement/tools/git/","text":"Git enables you to track the changes made to files over time — specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt-get install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Setup Once Git is installed, configure it with your name and email address. This lets Git know who you are so that it can associate you with the commits you make. git config --global user.name \"John Doe\" git config --global user.email johndoe@example.com Basic Commands git init Tells git to enable tracking of changes that happen in this folder. git clone <url> | <user@server:path/to/repo.git> Makes a full copy of an existing git repository — all files, folders, changes, history, etc. git status Lists which files are in which state — if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made and select which will be staged . git commit Commits all the staged changes (done with git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Stages the file to be removed. After you commit, the file will be removed and no longer tracked. But the file does remain in the project history. git mv <file-from> <file-to> Moves/renames a file. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and will need to be installed ( apt-get install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching if you're still unsure about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","title":"Git"},{"url":"datamanagement/tools/datalad/","text":"Datalad is available from https://github.com/datalad/datalad and its documentation can be found at http://docs.datalad.org .","tags":"tools","title":"Datalad"},{"url":"datamanagement/tools/webui/","text":"","tags":"tools","title":"WebUI"},{"url":"datamanagement/tools/toolbox/","text":"The Toolbox Dataset for datalad-hirni is available from https://github.com/psychoinformatics-de/hirni-toolbox . Usually it should automatically be installed when setting up a study dataset . Note, that by default the toolbox is installed from github by datalad run-procedure setup_hirni_dataset . If you need to install from elsewhere, you can set the datalad.hirni.toolbox.url config to point to another URL prior to running setup_hirni_dataset .","tags":"tools","title":"Toolbox Dataset"},{"url":"analysis/","text":"This is work-in-progress.","tags":"pages","title":"Data Analysis"},{"url":"devices/","text":"The pages in this section contain information on data acquisition devices (to be) supported in the CBBS imaging platform.","tags":"pages","title":"Acquisition Hardware"},{"url":"devices/magnetom/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"7T Siemens Magnetom"},{"url":"devices/achieva/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Philips Achieva"},{"url":"devices/prisma/","text":"Platform compatibility DICOM import DICOM metadata extraction and indexing","tags":"devices","title":"3T Siemens Prisma"},{"url":"devices/skyra/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Skyra"},{"url":"devices/verio/","text":"Platform compatibility This device is similar to the Siemens Prisma , hence it should feature the same compatibility level. However, no explicit tests have been made yet.","tags":"devices","title":"3T Siemens Verio"},{"url":"devices/eyelink/","text":"MR-compatible Eyelink 1000 (responsible: Stefan Pollmann). Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Eyelink 1000"},{"url":"devices/stadlerphysio/","text":"This is Jörg's physio recording box that can be used with the Siemens MR scanners. Platform compatibility Import scripts are available, but no dedicated tests have been performed yet.","tags":"devices","title":"Physio box"},{"url":"containers/","text":"In order to faciliate the reproducibility of research computing, all data processing is performed in containerized computational environments. These environments can be archived alongside the results of the data processing algorithms they have performed, in order to generate a complete record of the processing history. Moreover, such computional environment can be tracked with version control systems to capture which particular results were produced with which software version, in case data processing pipelines have to be adjusted throughout the lifetime of a project. The CBBS imaging platform employs Singularity as the main workhorse for container-based data processing.","tags":"pages","title":"Computational Environments"},{"url":"contributing/","text":"This project welcomes contributions from platform users and developers. The following list provides information on how to contribute to individual aspects of the imaging platform: Contribute to this documentation Contribute an implementation of a new analysis procedure In general, all documentation and source code are available under free and open source licences in various repositories on GitHub . Any contributions in the form of reported issues, or patches are much appreciated.","tags":"pages","title":"Contributing"},{"url":"contributing/docs/","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. If your proposed fix is limited to content, then you can probably skip setting up the build system. You can simply make your changes (content is written in reStructuredText ) and submit a Pull Request . The website is generated by Pelican and the template is written using Jinja2 . So, if your proposed changes are more widespread or change the template, you'll need to setup the build environment. Don't worry; it's easy. install pelican ( apt-get install pelican on Debian; pip install --user pelican on other OSs.) install beautifulsoup4 (again: apt-get install python-beautifulsoup or pip install --user beautifulsoup4 respectively), which is required by some pelican plugins we use run make devserver open a browser to http://127.0.0.1:8000 And that's it. Any changes you make will automatically trigger a rebuild.","tags":"contributing","title":"Documentation"}]}