{"pages":[{"url":"datamanagement/","text":"The fundamental concept of this platform is that data are first transformed into a standard layout using common file formats, so that all processing pipelines can expect a deterministic, well described data structure. This setup enables the development of data processing components that are agnostic of the peculiarities of individual studies, as long as all relevant aspects of a study are properly described. The necessary data conversion and layout are performed largely automatic. The procedures are tuned for data acquired in Magdeburg, but with minor extra effort data form other sources can be processed too. For each study the following steps will be performed: Initial study setup: create a study raw dataset Import a new acquisition (repeated as necessary): Import a DICOM dataset Import other data modalities Complete study description (if necessary): Identify data that were acquired simultaneously during an acquisition Associate stimulation and behavior logs with (MR) data acquisitions Identify data formats/converters for non-standard acquisitions (e.g. custom hardware) Convert raw data into the common data structure Verify result of automatic conversion (automatic validation tools are available) Apply analysis procedure","tags":"pages","title":"Data Management"},{"url":"datamanagement/study_setup/","text":"A study raw dataset contains all raw data and metadata on a study, across all acquisitions and participants. It is typically created just prior the first (pilot) acquisition. Its purpose is to record all data in their raw, unaltered form, as provided by the respective acquisition machinery. Moreover, it is used to bind data from multiple acquisition sources (e.g. MRI, eye tracker, ECG) together, to create a reliable record of what data were recorded at which point on what hardware. Importantly, this also includes the logs of any stimulation setup, or behavior responses that are typically generated by paradigm implementations provided by individual researchers. The study raw dataset should capture all information required to reproduce any subsequently performed data analyses. Preparation Start off by installing https://github.com/psychoinformatics-de/cbbs-imaging-container-import , which also provides additional tools apart from sheer creation: datalad install -s https://github.com/psychoinformatics-de/cbbs-imaging-container-import [INSTALL-DIR] After successful installation run the following command to retrieve the actual container image: datalad get INSTALL-DIR/cbbs-imaging.simg Dataset creation To create a study dataset you just need to run: singularity run INSTALL-DIR/cbbs-imaging.simg create [TARGET-DIR] . If you don't provide a target dir, the dataset will be created in the current working directory. Adding an MRI session The above mentioned container also provides a command to import a tarball for a session. This will result in a subdataset with the DICOMs in it, that provides the DICOM metadata for easy access via datalad . To import such a tarball just run singularity run [--bind=/path/to/tarball] INSTALL-DIR/cbbs-imaging.simg import [TARBALL] from within your study dataset. You don't need to be at its root directory, but you need to make sure, you aren't accidentally in a subdataset. Therefore the study dataset's root is a safe practice to follow. Note, that TARBALL needs to be an absolute path at the moment. Particularly if TARBALL is located outside your HOME directory, you may need to provide its location to the container via the --bind option. A session name like af29_0678 will be derived from the dicoms' metadata and a subdirectory with that is created. In that subdirectory you should then find a studyspec.json , which is needed for conversion later on and a subdirectory dicoms , which is in fact a subdataset containing the actual dicoms.","tags":"datamanagement","title":"Initial study setup"},{"url":"datamanagement/import_dicoms/","text":"The above mentioned container also provides a command to import a tarball for a session. This will result in a subdataset with the DICOMs in it, that provides the DICOM metadata for easy access via datalad . To import such a tarball just run singularity run [--bind=/path/to/tarball] INSTALL-DIR/cbbs-imaging.simg import [TARBALL] from within your study dataset. You don't need to be at its root directory, but you need to make sure, you aren't accidentally in a subdataset. Therefore the study dataset's root is a safe practice to follow. Note, that TARBALL needs to be an absolute path at the moment. Particularly if TARBALL is located outside your HOME directory, you may need to provide its location to the container via the --bind option. A session name like af29_0678 will be derived from the dicoms' metadata and a subdirectory with that is created. In that subdirectory you should then find a studyspec.json , which is needed for conversion later on and a subdirectory dicoms , which is in fact a subdataset containing the actual dicoms.","tags":"datamanagement","title":"Import DICOMs"},{"url":"datamanagement/conversion/","text":"","tags":"datamanagement","title":"Raw Data Conversion"},{"url":"datamanagement/tools/","text":"DataLad Git","tags":"datamanagement","title":"Tools"},{"url":"datamanagement/demo_scandb/","text":"The following script shows how a bunch of DICOM tarballs from two different scanners can be imported into a DataLad superdataset for each scanner. Those two scanner datasets are than assembled into a joint superdataset for acquisition hardware of the LIN. Metadata from any acquisition session can then be aggregated into this dataset, to track all acquisitions made on those devices, as well as to be able to query for individual scan sessions, DICOM series, or individual DICOM images. Such query only require the presence of metadata, and do not depend on the availability of actual raw data. # get the tools datalad install -s https://github.com/psychoinformatics-de/cbbs-imaging-container-import import datalad get import/cbbs-imaging.simg # create a super dataset that will have all acquisitions the 7T ever made singularity run import/cbbs-imaging.simg create 7t cd 7t # import a bunch of DICOM tarballs (simulates daily routine) singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/pandora/data/yx11/raw/dicom/yx11_2022.20130410.103515.930000.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/yx11/raw/dicom/yx11_2014.20130408.123933.087500.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/7T_ad/data/yx11/raw/dicom/yx11_2015.20130408.140515.147500.tar.gz # done for now cd .. # now the same for 3t singularity run import/cbbs-imaging.simg create 3t cd 3t # import a bunch of DICOM tarballs singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/yx11_0138.20140425.121603.06.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_av_et/mri/yx11_0139.20140425.142752.07.tar.gz singularity run ../import/cbbs-imaging.simg import \\ /home/data/psyinf/forrest_gump/3T_visloc/mri/yx11_0140.20140425.155736.23.tar.gz # done cd .. # one dataset for the entire institute's scan (could in turn be part of one that also # includes other modalities/machines) # this first part only needs to be done once datalad create lin cd lin datalad install -d . -s ../7t datalad install -d . -s ../3t # this second part needs to be done everytime the metadata DB shall be updated # get the latest state of the scanner datasets (no heavy stuff is moved around) datalad update --merge -r # aggregate from the aggregated metadata datalad aggregate-metadata -r # ready to search","tags":"datamanagement","title":"Demo: DICOM DB"},{"url":"datamanagement/tools/datalad/","text":"","tags":"tools","title":"Datalad"},{"url":"datamanagement/tools/git/","text":"Git enables you to track the changes made to files over time — specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt-get install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Setup Once Git is installed, configure it with your name and email address. This lets Git know who you are so that it can associate you with the commits you make. git config --global user.name \"John Doe\" git config --global user.email johndoe@example.com Basic Commands git init Tells git to enable tracking of changes that happen in this folder. git clone <url> | <user@server:path/to/repo.git> Makes a full copy of an existing git repository — all files, folders, changes, history, etc. git status Lists which files are in which state — if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made and select which will be staged . git commit Commits all the staged changes (done with git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Stages the file to be removed. After you commit, the file will be removed and no longer tracked. But the file does remain in the project history. git mv <file-from> <file-to> Moves/renames a file. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and will need to be installed ( apt-get install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching if you're still unsure about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","title":"Git"},{"url":"analysis/","text":"","tags":"pages","title":"Data Analysis"},{"url":"devices/","text":"Add general info on those devices, whom to contact, possible limitations of support wrt the services provided.","tags":"pages","title":"Acquisition Hardware"},{"url":"devices/magnetom/","text":"This is just a placeholder.","tags":"devices","title":"7T Siemens Magnetom"},{"url":"devices/achieva/","text":"This is just a placeholder.","tags":"devices","title":"3T Philips Achieva"},{"url":"devices/prisma/","text":"This is just a placeholder.","tags":"devices","title":"3T Siemens Prisma"},{"url":"devices/skyra/","text":"This is just a placeholder.","tags":"devices","title":"3T Siemens Skyra"},{"url":"devices/verio/","text":"This is just a placeholder.","tags":"devices","title":"3T Siemens Verio"},{"url":"devices/eyelink/","text":"This is just a placeholder.","tags":"devices","title":"Eyelink 1000"},{"url":"devices/stadlerphysio/","text":"This is Jörg's physio recording box for the Siemens machines. We need to ask him what we should call it.","tags":"devices","title":"Physio box"},{"url":"containers/","text":"In order to faciliate the reproducibility of research computing, all data processing is performed in containerized computational environments. These environments can be archived alongside the results of the data processing algorithms they have performed, in order to generate a complete record of the processing history. Moreover, such computional environment can be tracked with version control systems to capture which particular results were produced with which software version, in case data processing pipelines have to be adjusted throughout the lifetime of a project. The CBBS imaging platform employs Singularity as the main workhorse for container-based data processing.","tags":"pages","title":"Computational Environments"},{"url":"containers/rawimport/","text":"This Singularity container contains all software necessary to perform raw data import for all supported data types, and for conversion into the common data structure. Development information Report issues","tags":"containers","title":"Raw data import"},{"url":"contributing/","text":"Contribute to this documentation","tags":"pages","title":"Contributing"},{"url":"contributing/docs/","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. If your proposed fix is limited to content, then you can probably skip setting up the build system. You can simply make your changes (content is written in reStructuredText ) and submit a Pull Request . The website is generated by Pelican and the template is written using Jinja2 . So, if your proposed changes are more widespread or change the template, you'll need to setup the build environment. Don't worry; it's easy. install pelican ( apt-get install pelican on Debian; pip install --user pelican on other OSs.) install beautifulsoup4 (again: apt-get install python-beautifulsoup or pip install --user beautifulsoup4 respectively), which is required by some pelican plugins we use run make devserver open a browser to http://127.0.0.1:8000 And that's it. Any changes you make will automatically trigger a rebuild.","tags":"contributing","title":"Documentation"}]}